temp <- read.csv(fname)  # прочитаем файл с очередным именем
all_data <- rbind(all_data, temp)  # подклеим прочитанную табличку в конец таблички all_data
}
# в этот момент нужно установить рабочую папку Session - Set working directory -
# To source file location читаем данные из файла в таблицу h
h <- read.table("flats_moscow.txt", header = TRUE)
# смотрим, что всё корректно загрузилось
head(h)  # носик
tail(h)  # хвостик
qplot(data = h, x = totsp, y = price)  # диаграмма рассеяния
# на первом шаге оценим модель с помощью МНК проигнорировав возможную
# гетероскедастичность
model <- lm(price ~ totsp, data = h)  # простая модель парной регрессии
summary(model)  # отчет по модели
coeftest(model)  # тесты незначимости коэффициентов
confint(model)  # доверительные интервалы
# добавляем в исходную таблицу h прогнозы, остатки из модели model
h <- augment(model, h)
glimpse(h)  # смотрим, что добавилось в таблицу h
# строим зависимость модуля (функция abs) остатков от общей площади
qplot(data = h, totsp, abs(.resid))
# простая оценка ковариационной матрицы верная в условиях гомоскедастичности
# неверная в условиях гетероскедастичности
vcov(model)
# робастная оценка ковариационной матрицы устойчивая к гетероскедастичности
vcovHC(model, type = "HC0")  # формула Уайта
vcovHC(model)  # современный вариант формулы Уайта 'HC3'
vcovHC(model, type = "HC2")  # еще один вариант
# проверяем незначимость коэффициентов с помощью:
coeftest(model)  # обычной оценки ковариационной матрицы
# робастной оценки ковариационной матрицы:
coeftest(model, vcov. = vcovHC(model))
# сначала сохраним таблицу с коэффициентами и робастными ст. ошибками
conftable <- coeftest(model, vcov. = vcovHC(model))
# возьмем из этой таблицы два столбика (1-ый и 2-ой) и поместим в таблицу ci
ci <- data.frame(estimate = conftable[, 1], se_hc = conftable[, 2])
ci  # глянем на таблицу ci
# добавим в ci левую и правую границу доверительного интервала
ci <- mutate(ci, left_ci = estimate - 1.96 * se_hc, right_ci = estimate + 1.96 *
se_hc)
ci  # смотрим на результат
# для сравнение доверительные интервалы
confint(model)  # по формулам корректным для гомоскедастичности
# тест Бройша-Пагана Во вспомогательной регрессии квадраты остатков зависят от
# исходных регрессоров
bptest(model)
# тест Уайта Во вспомогательной регрессии квадраты остатков зависят от totsp и
# totsp^2
bptest(model, data = h, varformula = ~totsp + I(totsp^2))
# альтернативный вариант включить totsp и totsp^2
bptest(model, data = h, varformula = ~poly(totsp, 2))
# тест Голдфельда-Квандта
gqtest(model, order.by = ~totsp, data = h, fraction = 0.2)
# диаграмма рассеяния в логарифмах
qplot(data = h, log(totsp), log(price))
# модель парной регрессии в логарифмах
model2 <- lm(data = h, log(price) ~ log(totsp))
# тест Голдфельда-Квандта для модели в логарифмах
gqtest(model2, order.by = ~totsp, data = h, fraction = 0.2)
library("lubridate")  # работа с датами
library("sandwich")  # vcovHC, vcovHAC
library("lmtest")  # тесты
library("car")  # еще тесты
library("zoo")  # временные ряды
library("xts")  # еще ряды
library("dplyr")  # манипуляции с данными
library("broom")  # манипуляции
library("ggplot2")  # графики
library("quantmod")  # загрузка с finance.google.com
library("rusquant")  # загрузка с finam.ru
library("sophisthse")  # загрузка с sophist.hse.ru
library("Quandl")  # загрузка с Quandl
# задаём даты в виде простого текста
x <- c("2012-04-15", "2011-08-17")
y <- ymd(x)  # конвертируем в специальный формат дат
y
y + days(20)  # прибавим 20 дней
y - years(10)  # вычтем 10 лет
day(y)  # вытащим из даты только число
month(y)  # ... только месяц
year(y)  # ... только год
vignette("lubridate")  # более подробная справка про даты
# создадим временной ряд
x <- rnorm(5)  # пять N(0,1) случайных величин
x
y <- ymd("2014-01-01") + days(0:4)  # даты к этим величинам
y
ts <- zoo(x, order.by = y)  # склеим числа и даты в один временной ряд
ts
# функция lag есть в разных пакетах, и действует, она, увы, по-разному
# функция lag из пакета stats:
stats::lag(ts, -1)  # лаг, то есть прошлое значение ряда
stats::lag(ts, 1)  # форвардный лаг, то есть будущее значение
# функция lag из пакета dplyr
dplyr::lag(ts, -1)  # не должна сработать
dplyr::lag(ts, 1)  # лаг, то есть прошлое значение ряда
# сравните с др
diff(ts)  # приращение ряда
# те же пять чисел, только оформленные как квартальные данные
ts2 <- zooreg(x, start = as.yearqtr("2014-01"), freq = 4)
ts2
# те же пять чисел, только оформленные как месячные данные
ts3 <- zooreg(x, start = as.yearmon("2014-01"), freq = 12)
ts3
data("Investment")  # встроенный набор данных
help("Investment")
start(Investment)  # момент начала временного ряда
end(Investment)  # окончания
time(Investment)  # только моменты времени
coredata(Investment)  # только сами числа без дат
dna <- Investment  # скопируем набор данных Investment
dna[1, 2] <- NA  # и внесем туда искусственно пропуски
dna[5, 3] <- NA
na.approx(dna)  # линейная аппроксимация
na.locf(dna)  # заполнение последним известным значением
# загрузка данных с sophist.hse.ru это численность населения России
a <- sophisthse("POPNUM_Y")
# загрузка данных с quandl
b <- Quandl("FRED/GNP")
b
# загрузка данных finance.google.com
Sys.setlocale("LC_TIME", "C")  # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL", from = "2010-01-01", to = "2014-02-03", src = "google")
library("dplyr")  # манипуляции с данными
library("erer")  # расчет предельных эффектов
library("vcd")  # графики для качественных данных
library("ggplot2")  # графики
library("reshape2")  # манипуляции с данными
library("AUC")  # для ROC кривой
# при загрузке файлов R автоматом переделывает все строковые переменные в
# факторные эта шаманская команда просит R так не делать :)
options(stringsAsFactors = FALSE)
# читаем данные по пассажирам Титаника
t <- read.csv("titanic3.csv")
setwd("~/Documents/GitHub/Econometrics/coursera_R/week_07/sem_07")
# читаем данные по пассажирам Титаника
t <- read.csv("titanic3.csv")
# смотрим на набор данных
glimpse(t)
# объясняем R, какие переменные считать факторными
t <- mutate(t, sex = as.factor(sex), pclass = as.factor(pclass), survived = as.factor(survived))
summary(t)
# мозаичный график
mosaic(data = t, ~sex + pclass + survived, shade = TRUE)
# график-виолончель
qplot(data = t, x = survived, y = age, geom = "violin")
# и старый добрый 'ящик с усами'
qplot(data = t, x = survived, y = age, geom = "boxplot")
# два варианта сглаженной функции плотности
ggplot(data = t, aes(x = age, y = ..count.., fill = survived)) + geom_density(position = "stack")
ggplot(data = t, aes(x = age, y = ..count.., fill = survived)) + geom_density(position = "fill")
# Оценивание логит и пробит моделей
m_logit <- glm(data = t, survived ~ sex + age + pclass + fare, family = binomial(link = "logit"),
x = TRUE)
m_probit <- glm(data = t, survived ~ sex + age + pclass + fare, family = binomial(link = "probit"),
x = TRUE)
# отчеты об оценке моделей
summary(m_logit)
summary(m_probit)
# оценка ковариационной матрицы оценок коэффициентов
vcov(m_logit)
# создаём новый массив данных для прогнозирования
newdata <- data.frame(age = seq(from = 5, to = 100, length = 100), sex = "male",
pclass = "2nd", fare = 100)
# посмотрим на начало этой таблички
head(newdata)
# прогнозируем по логит модели
pr_logit <- predict(m_logit, newdata, se = TRUE)
# соединим прогнозы и новый массив данных в единую табличку:
newdata_pr <- cbind(newdata, pr_logit)
head(newdata_pr)  # глянем на начало таблички
# применив логистическую функцию распределения получим границы доверительного
# интервала
newdata_pr <- mutate(newdata_pr, prob = plogis(fit), left_ci = plogis(fit - 1.96 *
se.fit), right_ci = plogis(fit + 1.96 * se.fit))
head(newdata_pr)  # глянем на результат
# посмотрим на графике как меняется доверительный интервал для вероятности
qplot(data = newdata_pr, x = age, y = prob, geom = "line") + geom_ribbon(aes(ymin = left_ci,
ymax = right_ci), alpha = 0.2)
# проведем LR тест R при построении разных моделей автоматом использует
# максимальное количество полных наблюдений поэтому часто выходит, что
# ограниченная и неограниченная модель оцениваются на разном наборе данных но в
# таком случае их нельзя сравнивать с помощью LR теста поэтому мы сначала
# создадим набор данных t2 без пропущенных значений и на нем оценим короткую и
# длинную модели H0: beta(pclass)=0, beta(fare)=0
t2 <- dplyr::select(t, sex, age, pclass, survived, fare) %>% na.omit()
# оцениваем ограниченную модель
m_logit2 <- glm(data = t2, survived ~ sex + age, family = binomial(link = "logit"),
x = TRUE)
# проводим LR тест
lrtest(m_logit, m_logit2)
maBina(m_logit)  # предельные эффекты для среднестатистического пассажира
# усредненные предельные эффекты по всем пассажирам
maBina(m_logit, x.mean = FALSE)
# обычный МНК
m_ols <- lm(data = t, as.numeric(survived) ~ sex + age + pclass + fare)
summary(m_ols)
# прогнозы по обычному МНК
pr_ols <- predict(m_ols, newdata)
head(pr_ols)
# ROC кривая спрогнозируем скрытую переменную для исходного набора данных
pr_t <- predict(m_logit, t, se = TRUE)
# соединим прогнозы с исходным набором данных
t <- cbind(t, pr_t)
# применим логистическую функцию распределения, чтобы получить вероятности
t <- mutate(t, prob = plogis(fit))
# глянем выборочно на результат:
select(t, age, survived, prob)
# глянем выборочно на результат:
dplyr::select(t, age, survived, prob)
# получим все данные для ROC кривой:
roc.data <- roc(t$prob, t$survived)
str(roc.data)
# три графика для выбора порога отсечения по горизонтали --- пороги, по вертикали
# --- чувствительность чувствительность = число выживших верно предсказанных
# выжившими / общее количество выживших
qplot(x = roc.data$cutoffs, y = roc.data$tpr, geom = "line")
# по горизонтали --- пороги, по вертикали --- процент ложноположительных
# прогнозов процент ложно положительных прогнозов = число погибших ошибочно
# предсказанных выжившими/общее число погибших
qplot(x = roc.data$cutoffs, y = roc.data$fpr, geom = "line")
# по горизонтали --- процент ложноположительных прогнозов по вертикали ---
# чувствительность
qplot(x = roc.data$fpr, y = roc.data$tpr, geom = "line")
library("lubridate")  # работа с датами
library("zoo")  # временные ряды
library("xts")  # еще ряды
library("dplyr")  # манипуляции с данными
library("ggplot2")  # графики
library("forecast")
library("quantmod")  # загрузка с finance.google.com
library("sophisthse")  # загрузка с sophist.hse.ru
# симулируем процесс AR(1) y_t=0.7y_{t-1}+\e_t
y <- arima.sim(n = 100, list(ar = 0.7))
plot(y)  # график ряда
Acf(y)
Pacf(y)
tsdisplay(y)  # все три графика одним махом
# симулируем процесс MA(1) y_t=\e_t-0.8\e_{t-1}
y <- arima.sim(n = 100, list(ma = -0.8))
tsdisplay(y)
# симулируем процесс ARMA(1,1) y_t=0.5y_{t-1}+\e_t-0.8\e_{t-1}
y <- arima.sim(n = 100, list(ma = -0.8, ar = 0.5))
tsdisplay(y)
# симулируем процесс ARMA(1,1) y_t=-0.5y_{t-1}+\e_t-0.8\e_{t-1}
y <- arima.sim(n = 100, list(ma = -0.8, ar = -0.5))
tsdisplay(y)
# симулируем процесс случайного блуждания y_t=y_{t-1}+\e_t
y <- arima.sim(n = 100, list(order = c(0, 1, 0)))
tsdisplay(y)
# симулируем процесс случайного блуждания y_t=y_{t-1}+\e_t
y <- arima.sim(n = 100, list(order = c(0, 1, 0)))
tsdisplay(y)
# то же случайное блуждание, только 501 наблюдение
y <- arima.sim(n = 500, list(order = c(0, 1, 0)))
tsdisplay(y)
# добавим в AR(1) процессу тренд
y <- seq(0, 10, length = 100) + arima.sim(n = 100, list(ar = 0.7))
tsdisplay(y)
# добавим тренд послабее к AR(1) процессу
y <- seq(0, 2, length = 100) + arima.sim(n = 100, list(ar = 0.7))
tsdisplay(y)
# динамика уровня воды в озере Гурон
y <- LakeHuron
tsdisplay(y)
# оценим AR(2)
mod_1 <- Arima(y, order = c(2, 0, 0))
# оценим ARMA(1,1)
mod_2 <- Arima(y, order = c(1, 0, 1))
# результаты оценивания:
summary(mod_1)
summary(mod_2)
# штрафной критерий AIC
AIC(mod_1)
AIC(mod_2)
# оценим ARMA(2,1)
mod_3 <- Arima(y, order = c(2, 0, 1))
summary(mod_3)
AIC(mod_3)
# прогнозируем по модели 2 на 5 шагов вперед
prognoz <- forecast(mod_2, h = 5)
prognoz
# строим график прогноза
plot(prognoz)
# оценим ARIMA(1,1,0)
mod_4 <- Arima(y, order = c(1, 1, 0))
AIC(mod_4)
# прогноз по модели ARIMA(1,1,0)
prognoz_4 <- forecast(mod_4, h = 5)
plot(prognoz_4)
# автоматический подбор модели по штрафному критерию
mod_a <- auto.arima(y)
summary(mod_a)
# прогноз по автоматически подбираемой модели
prognoz_a <- forecast(mod_a, h = 5)
plot(prognoz_a)
# шаманское заклинание для перевода дат на английский чтобы корректно работала
# загрузка данных под русскоязычной windows
Sys.setlocale("LC_TIME", "C")
# загружаем данные по стоимости акций Гугла
getSymbols(Symbols = "GOOGL", from = "2014-01-01", to = "2016-05-11", src = "google")
# загружаем данные по стоимости акций Гугла
getSymbols(Symbols = "GOOGL", from = "2014-01-01", to = "2016-05-11", src = "yahoo")
head(GOOG)  # начало набора данных
y <- GOOG$GOOG.Close  # поместим цену закрытия в переменную y
head(GOOGL)  # начало набора данных
y <- GOOGL$GOOGL.Close  # поместим цену закрытия в переменную y
tsdisplay(y)  # три графика для исходного ряда
dy <- diff(y)
tsdisplay(dy)  # три графика для приращений исходного ряда (для первой разности)
# похоже на случайное блуждание, оценим ARIMA(0,1,0)
mod_1 <- Arima(y, order = c(0, 1, 0))
summary(mod_1)
# прогнозируем на 20 шагов вперед
prognoz_1 <- forecast(mod_1, h = 20)
prognoz_1
tsdisplay(y)  # три графика для исходного ряда
dy <- diff(y)
tsdisplay(dy)  # три графика для приращений исходного ряда (для первой разности)
# похоже на случайное блуждание, оценим ARIMA(0,1,0)
mod_1 <- Arima(y, order = c(0, 1, 0))
summary(mod_1)
# прогнозируем на 20 шагов вперед
prognoz_1 <- forecast(mod_1, h = 20)
prognoz_1
# строим график прогноза
plot(prognoz_1)
# автоматический подбор модели
mod_a <- auto.arima(y)
summary(mod_a)
# численность населения России
y <- sophisthse("POPNUM_Y")
tsdisplay(y)
# ARIMA(1,1,0) со смещением
mod_1 <- Arima(y, order = c(1, 1, 0), include.drift = TRUE)
summary(mod_1)
# прогноз на 5 шагов вперед
prognoz_1 <- forecast(mod_1, h = 5)
plot(prognoz_1)
# индекс потребительских цен (ежемесячные данные)
y <- sophisthse("CPI_M_CHI")
install.packages("devtools")
devtools::install_github("bdemeshev/sophisthse")
library("dplyr")  # манипуляции с данными
library("caret")  # стандартизованный подход к регрессионным и классификационным моделям
library("AER")  # инструментальные переменные
library("ggplot2")  # графики
library("sandwich")  # робастные стандартные ошибки
library("ivpack")  # дополнительные плющки для инструментальных переменных
library("memisc")  # табличка mtable
# прочитаем данные из .txt файла есть заголовок, header = TRUE разделитель данных
# - табуляция, sep='\t' разделитель дробной части - точка, dec='.'
h <- read.csv("flats_moscow.txt", header = TRUE, sep = "\t", dec = ".")
setwd("~/Documents/GitHub/Econometrics/coursera_R/week_09/sem_09")
# прочитаем данные из .txt файла есть заголовок, header = TRUE разделитель данных
# - табуляция, sep='\t' разделитель дробной части - точка, dec='.'
h <- read.csv("flats_moscow.txt", header = TRUE, sep = "\t", dec = ".")
glimpse(h)  # бросим взгляд на данные
# добавим логарифмы цены и площадей
h2 <- mutate(h, logprice = log(price), logtotsp = log(totsp), logkitsp = log(kitsp),
loglivesp = log(livesp))
# создадим разбиение данных, отберем 75% случайных номеров
in_train <- createDataPartition(y = h2$logprice, p = 0.75, list = FALSE)
h2_train <- h2[in_train, ]  # отберем обучающую часть выборки
h2_test <- h2[-in_train, ]  # оставшееся пойдет в тестовую часть выборки
# оценим две модели с помощью МНК
model_1 <- lm(data = h2_train, logprice ~ logkitsp + logtotsp + loglivesp)
model_2 <- lm(data = h2_train, logprice ~ logtotsp)
# построим прогнозы по двум моделям на тестовой выборке
pred_1 <- predict(model_1, h2_test)
pred_2 <- predict(model_2, h2_test)
# посчитаем руками суммы квадратов остатков по тестовой выборке
sum((pred_1 - h2_test$logprice)^2)
sum((pred_2 - h2_test$logprice)^2)
## данные
data("CigarettesSW", package = "AER")  # активируем набор данных
help("CigarettesSW")  # читаем справку
# для удобства назовем покороче
h <- CigarettesSW
glimpse(h)  # посмотрим на структуру данных
# построим диаграмму рассеяния
qplot(data = h, price, packs)
# отберем данные относящиеся к 1995 году
h2 <- filter(h, year == "1995")
# создадим новые переменные
h2 <- mutate(h2, rprice = price/cpi, rincome = income/cpi/population, tdiff = (taxs -
tax)/cpi)
# снова глянем на диаграмму рассеяния
qplot(data = h2, price, packs)
# и бросим взгляд на набор данных
glimpse(h2)
# оценим функцию спроса с помощью МНК забыв, что имеет место эндогенность
model_0 <- lm(data = h2, log(packs) ~ log(rprice))
summary(model_0)
# двухшаговый МНК руками Шаг 1. Строим регрессию эндогенного регрессора на
# инструментальную переменную
st_1 <- lm(data = h2, log(rprice) ~ tdiff)
# сохраняем прогнозы из регрессии первого шага
h2$logprice_hat <- fitted(st_1)
# Шаг 2. Строим регрессию зависимой переменной на прогнозы с первого шага
st_2 <- lm(data = h2, log(packs) ~ logprice_hat)
coeftest(st_2)
help(ivreg)  # документация по команде ivreg
# двухшаговый МНК в одну строчку
model_iv <- ivreg(data = h2, log(packs) ~ log(rprice) | tdiff)
coeftest(model_iv)  # здесь стандартные ошибки --- корректные
# сравним три модели в одной табличке
mtable(model_0, model_iv, st_2)
# используем для проверки гипотез робастные стандартные ошибки
coeftest(model_iv, vcov = vcovHC)
# модель с одной экзогенной, log(rincome), и одной эндогенной переменной,
# log(rprice)
iv_model_2 <- ivreg(data = h2, log(packs) ~ log(rprice) + log(rincome) | log(rincome) +
tdiff)
# тестируем гипотезы с использованием робастных стандартных ошибок
coeftest(iv_model_2, vcov = vcovHC)
# модель с одной экзогенной, одной эндогенной и двумя инструментальными
# переменными для эндогенной
iv_model_3 <- ivreg(data = h2, log(packs) ~ log(rprice) + log(rincome) | log(rincome) +
tdiff + I(tax/cpi))
# тестируем гипотезы с использованием робастных стандартных ошибок
coeftest(iv_model_3, vcov = vcovHC)
library("spikeslab")  # регрессия пик-плато
library("ggplot2")  # графики
library("dplyr")  # манипуляции с данными
library("reshape2")  # перевод таблиц: широкая-длинная
library("MCMCpack")  # байесовский подход к популярным моделям
library("quantreg")  # квантильная регрессия
library("randomForest")  # случайный лес
library("rattle")  # визуализация деревьев
library("caret")  # стандартный подход к регрессионным и классификационным задачам
library("rpart")  # классификационные и регрессионные деревья
# прочитаем данные из .txt файла есть заголовок, header = TRUE разделитель данных
# - табуляция, sep='\t' разделитель дробной части - точка, dec='.'
f <- read.table("flats_moscow.txt", header = TRUE, sep = "\t", dec = ".")
glimpse(f)  # бросим взгляд чтобы убедиться, что загрузка прошла успешно
# квантильная регрессия для квантилей 0.1, 0.5 (медианная), 0.9
model_q01 <- rq(data = f, price ~ totsp, tau = c(0.1, 0.5, 0.9))
summary(model_q01)  # отчет по модели
# базовый график --- диаграмма рассеяния
base <- qplot(data = f, totsp, price)
base
# добавляем к базовому графику две линии квантильной регрессии
base_q <- base + stat_smooth(method = "rq", method.args = list(tau = 0.1), se = FALSE) +
stat_smooth(method = "rq", method.args = list(tau = 0.9), se = FALSE)
base_q
# добавляем к графику дележку в зависимости от того, кирпичный дом или нет
base_q + aes(colour = factor(brick))
# делим набор данных на две части: обучающую (75%) и тестовую получаем номера
# наблюдений из обучающей части
set.seed(42) # для точной воспроизводимости случайного разбиения
in_sample <- createDataPartition(f$price, p = 0.75, list = FALSE)
head(in_sample)  # несколько номеров наблюдений, входящих в обучающую выборку
f_train <- f[in_sample, ]  # отбираем наблюдения с номерами из in_sample
f_test <- f[-in_sample, ]  # отбираем наблюдения с номерами не из in_sample
# обычная регрессия
model_lm <- lm(data = f_train, price ~ totsp + kitsp + livesp + brick)
# случайный лес
set.seed(42) # для точной воспроизводимости случайного леса
model_rf <- randomForest(data = f_train, price ~ totsp + kitsp + livesp + brick)
# поместим цену в отдельную переменную
y <- f_test$price
# прогнозы по МНК
yhat_lm <- predict(model_lm, f_test)
# прогнозы по случайному лесу
yhat_rf <- predict(model_rf, f_test)
# сумма квадратов остатков прогнозов по тестовой выборке: МНК
sum((y - yhat_lm) ^ 2)
# сумма квадратов остатков прогнозов по тестовой выборке: случайные лес
sum((y - yhat_rf) ^ 2)
# создаем плохой набор данных с 'идеальной классификацией'
bad <- data.frame(y = c(0, 0, 1), x = c(1, 2, 3))
bad
# классический логит в теории максимум правдоподобия не существует чем больше
# бета при икс, тем больше будет правдоподобия однако из-за ограниченной
# компьютерной точности R находит некоторые оценки
model_logit <- glm(data = bad, y ~ x, family = binomial(link = "logit"))
summary(model_logit)  # отчет по модели
# байесовский подход априорное распределение: beta ~ N(0, 50^2)
set.seed(42) # для точной воспроизводимости результатов MCMC
model_mcmc_logit <- MCMClogit(data = bad, y ~ x, b0 = 0, B0 = 1 / 50 ^ 2)
summary(model_mcmc_logit)  # отчет по байесовской логит-модели
# байесовский подход априорное распределение: beta ~ N(0, 10^2)
set.seed(42) # для точной воспроизводимости результатов MCMC
model_mcmc_logit <- MCMClogit(data = bad, y ~ x, b0 = 0, B0 = 1 / 10 ^ 2)
summary(model_mcmc_logit)  # отчет по байесовской логит-модели
# переведем скорость и длину тормозного пути во встроенном наборе данных в
# привычные единицы измерения
h <- mutate(cars, speed = 1.61 * speed, dist = 0.3 * dist)
# добавим квадрат скорости и мусорную переменную nrow(h) --- количество строк в
# таблице h
h <- mutate(h, speed2 = speed ^ 2, junk = rnorm(nrow(h)))
# обычная регрессия с мусорной переменной
model_lm <- lm(data = h, dist ~ speed + junk)
summary(model_lm)  # мусорная переменная незначима
# байесовская регрессия с мусорной переменной выборка из апостериорного
# распределения коэффициентов размера 4000
set.seed(42) # для точной воспроизводимости результатов MCMC
model_ss <- spikeslab(data = h, dist ~ speed + junk, n.iter2 = 4000)
print(model_ss)  # отчет по модели
model_ss$summary  # еще немного
# посмотрим, равнялся ли идеально нулю каждый коэффициент бета в выборке из
# апостериорного распределения
included_regressors <- melt(model_ss$model)
# глядим на полученный результат
head(included_regressors)
included_regressors
# сколько раз не был равен нулю бета при скорости / 4000
sum(included_regressors$value == 1)/4000
# сколько раз не был равен нулю бета при мусорной переменной / 4000
sum(included_regressors$value == 2)/4000
